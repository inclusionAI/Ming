# Ming

<p align="center">
    <img src="./Ming-omni/figures/ant-bailing.png" width="100"/>
<p>


This repository provides collections of Ming - facilitating advanced multimodal understanding and generation capabilities built upon the [Ling](https://github.com/inclusionAI/Ling) LLM.


## Introduction

Ming series comprises a range of MLLMs for multi-modal understanding and generation:
- Ming-Omni: It employs a unified Mixture-of-Experts (MoE) framework for multimodal sequence modeling, which empowers [Ling](https://github.com/inclusionAI/Ling) LLMs to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, Ming-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive realtime experience.
- Ming-Unify: It focuses on realizing a unified paradigm for multi-modal understanding and generation, aiming to extend the image and video generation capabilities while maintaining the multi-modal understanding abilities of MLLM.

We aim to progressively open-source the Ming series of multi-modal models, including not only the models and inference code but also the training code and datasets. This fosters a collaborative environment where researchers and developers can build upon and extend the models, driving innovation in AI applications. Through collaborative efforts with the community, we hope to gradually accelerate the development of the Ming series models, ultimately enabling the development of more advanced and practical AI systems that can tackle complex real-world problems.



## Contact Information

Please submit a GitHub issue if you want help or have issues using Ming.

## License

Ming is licensed under [the MIT License](./LICENSE).


